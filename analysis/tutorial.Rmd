---
title: "Tutorial New Features"
---

Setup.

```{r setup}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
knitr::opts_knit$set(root.dir = "~/Saez/workflowr_projects/benchmark")
```

Loaded packages.

```{r}
suppressPackageStartupMessages(library(mistyR))
suppressPackageStartupMessages(library(future))
suppressPackageStartupMessages(library(tidyverse))
plan("multisession", workers=14)
```

# Introduction

In this tutorial, I will briefly explain how I envision the new MISTy workflow.
Instead of only offering a pre-defined set of ML algorithms to model each 
view, I refactored the code such that users can supply their own function
if they would like. If not, we ship MISTy with several implemented functions
that can simply be plugged into the model. More details can be found below.

## Data

For the sake of this tutorial we will be using the synthetic dataset
supplied in the MISTy package. As explained in the 
[Get Started Vignette](https://saezlab.github.io/mistyR/articles/mistyR.html),
this dataset is based on a two-dimensional cellular automata model which 
models the production, diffusion, degradation and interactions of 11 
molecular species in 4 different cell types. 

In total there are 10 tibbles, each of which contains about 4000 cells and 
their corresponding expresssion of the markers, position in a 100x100 grid
(randomly assigned) as well as the cell type identity. (For more information
see `help("synthetic")`).

```{r}
data("synthetic")
```

## MISTy Views

For the sake of keeping things simply, we will only look at the first
instance of the synthetic dataset for now.

We will start by creating two MISTy views: a) Intraview and b) Paraview
with a Gaussian kernel (default) and a radius of 10.

```{r}
expr <- synthetic$synthetic1 %>% dplyr::select(-c(row, col, type))
pos <- synthetic$synthetic1 %>% dplyr::select(c(row, col))
misty.views <- expr %>%
  create_initial_view() %>%
  add_paraview(l = 10, positions = pos)
```

## Running MISty

The default ML algorithm is still random forest and thus we do not even
have to specify it.

```{r}
misty.run <- misty.views %>%
  run_misty()
```

To make things more explicitly the above call is the same as:

```{r}
misty.run <- misty.views %>%
  run_misty(model.function = ranger_model)
```

But what is this `ranger_model` actually?

```{r}
ranger_model
```

It is a function that takes in the data of a single view, for example the
`paraview` - 

```{r}
misty.views$paraview.10$data %>% slice_head(n=6)
```

and models the target variable with the RF algorithm. Importantly, within
the `run_misty` function before the `view_data` are supplied to the ranger_model,
the target column is replaced by the actual values from the `intraview`.

So assuming the target is "ECM" the following happens within the MISTy framework.

```{r}
target <- "ECM"
expr <- misty.views$intraview$data
target.vector <- expr %>% dplyr::pull(target)
view_data <- misty.views$paraview.10$data %>%
  mutate(!!target := target.vector)
# compare to the above
view_data %>% slice_head(n=6)
```

And then the RF algorithm is called with the default algorithms merged
with the additional ellipsis arguments.

```{r}
seed <- 42 # default

ellipsis.args <- list() # assume no ellipsis arguments were given

# default ranger arguments
algo.arguments <- list(
  formula = stats::as.formula(paste0(target, " ~ .")),
  data = view_data,
  num.trees = 100,
  importance = "impurity",
  mtry = NULL, 
  verbose = FALSE, 
  num.threads = 1,
  seed = seed)

if (!(length(ellipsis.args) == 0)) {
  algo.arguments <- merge_2(algo.arguments, ellipsis.args)
}

model <- do.call(ranger::ranger, algo.arguments)

predictions <- tibble::tibble(index = seq_len(nrow(view_data)), 
                              prediction = model$predictions)

list(unbiased.predictions = predictions, 
     importances = model$variable.importance) %>%
  str() # add str here to show output
```

(Note that `merge_2` is a function that is also exported by `mistyR`)

```{r echo=FALSE}
rm(view_data, target.vector, expr, model, ellipsis.args, algo.arguments)
```


As seen above, we can also easily supply arguments to the `ranger` RF implementation.
Say we would like to increase the number of trees and use another splitrule.
Let's compare the runtime for example (should be higher with more trees).

```{r}
tictoc::tic()
misty.run <- misty.views %>%
  run_misty(model.function = ranger_model, num.trees = 500, 
            splitrule = "extratrees")
tictoc::toc()
```

```{r}
tictoc::tic()
misty.run <- misty.views %>%
  run_misty(model.function = ranger_model, num.trees = 100, 
            splitrule = "extratrees")
tictoc::toc()
```

## MISTy results

On the side of processing and plotting the results nothing has changed.

```{r}
misty.results <- collect_results(misty.run)
misty.results %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

Below all the supplied function will be desribed in more detail.

# All Supplied Functions {.tabset}

## Random Forest

 - Unbiased predictions are based on OOB predictions.

```{r}
ranger_model
```

 - Running the model
 
```{r}
misty.views %>%
  run_misty(model.function = ranger_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```
 

## Bagged MARS

 - MARS:  Multivariate Adaptive Regression Splines

 - Unbiased predictions are based on OOB predictions.
 
 - The name `bagged_earth_model` comes from the fact that the name mars
 is protected and thus the implementation of the algorithm is called `earth` 
 (by Stephen Milborrow, derived from mda::mars by Trevor Hastie and Robert
 Tibshirani)

```{r}
bagged_earth_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = bagged_earth_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```
 

## Bagged Linear Model

 - Unbiased predictions are based on OOB predictions.

```{r}
bagged_linear_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = bagged_linear_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

## Boosted Trees

 - Unbiased predictions are based on k-fold cross-validation (aggregarted 
 predictions for the holdout sets)

```{r}
boosted_trees_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = boosted_trees_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

## Support Vector Machine

 - Unbiased predictions are based on k-fold cross-validation (aggregarted 
 predictions for the holdout sets)
 
 - Importantly, we added an approximation argument to the implementation which
 basically says at what fraction of the training instances the SVM will be
 trained on. This drastically decreases the training time and does not 
 really effect the performance.

```{r}
svm_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = svm_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

## Neural Network

 - Unbiased predictions are based on k-fold cross-validation (aggregarted 
 predictions for the holdout sets)
 
 - Importantly, the feature importances is here calculated based on a global 
 model agnostic method. More specifically each feature is permuated (one at a time)
 and then the reduction in preditive performance is used as measure for the
 variable importance.

```{r}
nn_model
```

 - Running the model:

```{r}
misty.views %>%
  run_misty(model.function = nn_model,
            k = 10, approx.frac = 0.5) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```

## Linear Model

 - Unbiased predictions are based on k-fold cross-validation (aggregated 
 predictions for the holdout sets)
 
 - In particular this model was supplied to understand more easily, how
 a view-specifc model based on cross-validation can be constructed

```{r}
linear_model
```

 - Running the model:
 
```{r}
misty.views %>%
  run_misty(model.function = linear_model) %>%
  collect_results() %>%
  plot_improvement_stats("gain.R2") %>%
  plot_view_contributions() %>%
  plot_interaction_heatmap("para.10")
```
 

# How to construct you own function

1. To construct a view-specific function we need to clearly state the
expected input and output to the function.

## Input

The input will be:

1. `view_data`: A tibble with each row corresponding to a spatial unit 
(e.g. Visium spot or single cell) and each column corresponding to a marker.

Based on the assert statements in `run_misty`, one can be sure that the 
variance of each target is non-zero.

For example the intraview from above.

```{r}
misty.views$intraview$data %>% slice_head(n=6)
```

2. `target`: String corresponding to the marker which should be modeled.

For example "ECM"

3. `seed`: Integer (passed down from `run_misty`)

Default seed is 42.

## Output

The output must be a named list comprising of:

1. `unbiased.predictions`: Tibble with one column called `index` for the "cell id"
and another column called `prediction` for the unbiased prediction.

These unbiased predictions for the specified target can either come from the
aggregated out-of-bag (OOB) predictions of a bagged ensembl model or the 
aggregated predictions for holdout set for k-fold cross-validation.

The unbiased predictions are needed as input for the linear fusion model,
which combined the unbiased predictions from each view-specific model to 
assess the contribution from each view (using the coefficients).

2. `importances` : Named Vector with the importances of the predictors as 
values and the names of the predictors as names (see the example below)

---

For example let's have a look at the result list returned by the ranger model.

```{r}
res.list <- ranger_model(view_data = misty.views$intraview$data, 
                         target = "ECM", 
                         seed = 42)
str(res.list)
```

a) Top 6 entries of the `unbiased.predictions` tibble.

```{r}
res.list$unbiased.predictions %>% slice_head(n = 6)
```

b) Importances

```{r}
res.list$importances
```

## Example Function

To showcase how one can construct such a function, we will build a 
view-specific model based on a decision tree (implemented by `rpart`)

To visualize what each step is necessary for, we will use the paraview from 
above and model "ECM". So in general our input is going to look somehow
like that. We have a tibble with the rows as spatial units (e.g. cells) 
and the columns are the markers (target or predictors).

```{r}
seed <- 42
target <- "ECM"
expr <- misty.views$intraview$data
target.vector <- expr %>% dplyr::pull(target)
view_data <- misty.views$paraview.10$data %>%
  mutate(!!target := target.vector)

view_data %>% slice_head(n=6)
```

So in the end the function should call `rpart` with this formula:

```{r}
as.formula(paste0(target, "~ ."))
```


```{r}
test.model <- rpart::rpart(formula = as.formula(paste0(target, "~ .")), 
                           data = view_data
)
summary(test.model)
```

Now to get unbiased predictions we need to do cross validation and aggregate 
the predictions for the holdout instances.

First we will create the 5 folds using `caret::createFolds`

```{r}
seed <- 42
k <- 5
folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
)
str(folds)
```

Next we will train a model for each fold and aggregate the prediction of the
holdout instances.

```{r}
holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
str(holdout.predictions)
```

As described above, apart from the unbiased predictions, we need the importances
of the predictors (interpretable model!). Therefore we will train one more
model on the whole dataset. And return the importances as named vector.

```{r}
algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
whole.model <- do.call(rpart::rpart, algo.arguments.wm)

importances <- whole.model$variable.importance
importances
```

In the end we need to return everything in one list with the following names:

```{r}
list(unbiased.predictions = holdout.predictions, 
       importances = importances) %>% str # again added str for visualization
```

So up to this point our function would look like this.

```{r}
reg_tree_model_1 <- function(view_data, seed = 42, k = 5) {
  
  folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
  )
  
  holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
  
  algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
  whole.model <- do.call(rpart::rpart, algo.arguments.wm)
  
  importances <- whole.model$variable.importance
  
  list(unbiased.predictions = holdout.predictions, 
       importances = importances)
}
```

And it works perfectly.

```{r}
test <- reg_tree_model_1(view_data = view_data)
str(test)
```

Now in the final step we will add the possibility to supply ellipsis arguments
to the `rpart` model.

```{r}
reg_tree_model_2 <- function(view_data, seed = 42, k = 5, ...) {
  
  ellipsis.args <- list(...)
  
  folds <- withr::with_seed(
  seed,
  caret::createFolds(seq.int(1, nrow(view_data)), k = k)
  )
  
  holdout.predictions <- purrr::map_dfr(folds, function(holdout) {
  
  in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
  
  train <- view_data[in.fold, ]
  test <- view_data[holdout, ]
  
  algo.arguments <- list(formula = as.formula(paste0(target, "~ .")), 
                        data = train)
  
  if (!(length(ellipsis.args) == 0)) {
    algo.arguments <- merge_2(algo.arguments, ellipsis.args)
  }
  
  model <- do.call(rpart::rpart, algo.arguments)
  
  label.hat <- predict(model, test)
  
  tibble::tibble(index = holdout, prediction = label.hat)
}) %>% dplyr::arrange(index)
  
  algo.arguments.wm <- list(formula = as.formula(paste0(target, "~ .")), 
                          data = view_data)
  
  if (!(length(ellipsis.args) == 0)) {
    algo.arguments.wm <- merge_2(algo.arguments.wm, ellipsis.args)
  }
  
  whole.model <- do.call(rpart::rpart, algo.arguments.wm)
  
  importances <- whole.model$variable.importance
  
  list(unbiased.predictions = holdout.predictions, 
       importances = importances)
}
```

Now check by supplying some parameters to `rpart.control`. For example
changing the complexity parameter `cp` from 0.01 to 0.001. In regression setting
this means that if a split does not decreae the $R^2$ by at least a factor of `cp`, 
the split is not attempted.

```{r}
test2 <- reg_tree_model_2(view_data = view_data, cp = 0.001)
str(test2)
```

For the sake of it we could compare the performance of both test models.

```{r}
# Model 1
caret::defaultSummary(data = data.frame(obs = target.vector,
                                        pred = test$unbiased.predictions$prediction))
# Model 2
caret::defaultSummary(data = data.frame(obs = target.vector,
                                        pred = test2$unbiased.predictions$prediction))
```

# Appendix

## What effect do the approximations have on the predictive performance?

The possibility for subsampling the training set was introduced to 
reduce the training time, since MISTy is often applied to large
datasets where the modelling of each view should not take too long.

Since the approximation only affects the computation of the unbiased 
prediction, we only need to have a look at the performance estimation

### SVM

```{r}
approx.results.svm <- map(seq(0.1, 1, l = 10), function(frac) {
  print(frac)
  misty.views %>%
  run_misty(model.function = svm_model, approx = TRUE, approx.frac = frac) %>%
  collect_results()
})
```

I guess based on those results a default approximation fraction of **0.4** would 
make sense.

```{r fig.height=6, fig.width=12}
svm.improvements <- map2_dfr(approx.results.svm, 
                             seq(0.1, 1, l = 10), function(misty.results, frac) {
  misty.results$improvements %>% mutate(approx.frac = frac)
})
svm.improvements %>%
  mutate(approx.frac = factor(approx.frac, levels = seq(0.1, 1, l = 10))) %>%
  filter(measure == "multi.R2") %>%
  ggplot() +
  geom_point(aes(x = approx.frac, y = value)) +
  facet_wrap(~ target, scales="free")
```

### NN

```{r}
approc.results.nn <- map(seq(0.1, 1, l = 10), function(frac) {
  print(frac)
  misty.views %>%
  run_misty(model.function = nn_model, approx = TRUE, approx.frac = frac) %>%
  collect_results()
})
```

I guess based on those results a default approximation fraction of **0.6** would 
make sense.

```{r fig.height=6, fig.width=12}
nn.improvements <- map2_dfr(approc.results.nn, 
                             seq(0.1, 1, l = 10), function(misty.results, frac) {
  misty.results$improvements %>% mutate(approx.frac = frac)
})
nn.improvements %>%
  mutate(approx.frac = factor(approx.frac, levels = seq(0.1, 1, l = 10))) %>%
  filter(measure == "multi.R2") %>%
  ggplot() +
  geom_point(aes(x = approx.frac, y = value)) +
  facet_wrap(~ target, scales="free")
```

## Overfitting the NN Function

```{r}
sizes = list("5" = c(5), "10" = c(10), "2x10" = c(10, 10), "3x10" = c(10, 10, 10),
             "3x16" = c(16, 16, 16))
overfit.nn.results <- map(sizes, function(s) {
  print(s)
  misty.views %>%
  run_misty(model.function = nn_model, approx = TRUE, approx.frac = 0.6,
            size = s) %>%
  collect_results()
})
```

And we basically see that the landscape is highly heterogeneous depending
on the target. Sometimes a more complex model is helpful and sometimes not! 

```{r fig.height=6, fig.width=12}
overfit.nn.impro <- map2_dfr(overfit.nn.results, 
                             names(sizes), function(misty.results, s) {
  misty.results$improvements %>% mutate(size = s)
})
overfit.nn.impro %>%
  filter(measure == "multi.R2") %>%
  mutate(size = factor(size, levels = names(sizes))) %>%
  ggplot() +
  geom_point(aes(x = size, y = value)) +
  facet_wrap(~ target, scales="free")
```

## How many variables to consider for bagged models

### Linear Bagged Model

```{r}
max.num <- misty.views$intraview$data %>% ncol - 1
nums <- floor(seq(2, max.num, l = 6))
num.test.lm <- map(nums, function(n) {
  misty.views %>%
    run_misty(model.function = bagged_linear_model, n.vars = n) %>%
    collect_results()
})
num.lm.performance <- map2_dfr(num.test.lm, nums, 
                            ~ .x$improvements %>% mutate(n.vars = .y))
```

```{r}
num.lm.performance %>%
  filter(measure == "multi.R2") %>%
  mutate(n.vars = factor(n.vars, levels = nums), sample = NULL, measure = NULL) %>%
  ggplot() +
  geom_point(aes(x = n.vars, y = value, col = n.vars)) +
  facet_wrap(~ target, scales = "free")
```

### Bagged MARS

```{r}
max.num <- misty.views$intraview$data %>% ncol - 1
nums <- floor(seq(2, max.num, l = 6))
num.test.mars <- map(nums, function(n) {
  misty.views %>%
    run_misty(model.function = bagged_earth_model, n.vars = n) %>%
    collect_results()
})
num.mars.performance <- map2_dfr(num.test.mars, nums, 
                            ~ .x$improvements %>% mutate(n.vars = .y))
```

```{r}
num.mars.performance %>%
  filter(measure == "multi.R2") %>%
  mutate(n.vars = factor(n.vars, levels = nums), sample = NULL, measure = NULL) %>%
  ggplot() +
  geom_point(aes(x = n.vars, y = value, col = n.vars)) +
  facet_wrap(~ target, scales = "free")
```

## Run Times

```{r}
functions <- c("RF" = ranger_model, "BT" = boosted_trees_model, 
               "NN" = nn_model, "SVM" = svm_model, 
               "MARS" = bagged_earth_model, "LM" = linear_model)
```

```{r}
timing <- map2_dfr(functions, names(functions), function(fun, fun.name) {
  start <- Sys.time()
  misty.views %>% run_misty(model.function = fun)
  end <- Sys.time()
  tibble::tibble(model.function = fun.name, time = (end - start))
})
rf <- timing %>% filter(model.function == "RF") %>% pull(time) %>% as.numeric
timing %>%
  arrange(desc(time)) %>% 
  mutate(rel.to.rf = round(as.numeric(time) / rf, 2))
```

```{r eval=FALSE, include=FALSE}
misty.views.smp <- map(synthetic, function(smp) {
  expr <- smp %>% select(-c(row, col, type))
  pos <- smp %>% select(c(row, col))
  misty.views <- create_initial_view(expr) %>%
    add_paraview(positions = pos, l = 10)
})

timing <- map2_dfr(functions, names(functions), function(fun, fun.name) {
  start <- Sys.time()
  misty.results.string <- map_chr(misty.views.smp, function(ms) {
    ms %>% run_misty(model.function = fun)
  })
  end <- Sys.time()
  tibble::tibble(model.function = fun.name, time = (end - start))
})
rf <- timing %>% filter(model.function == "RF") %>% pull(time) %>% as.numeric
timing %>%
  arrange(desc(time)) %>% 
  mutate(rel.to.rf = round(as.numeric(time) / rf, 2))
```

## Hyperparameters

### Interaction Terms MARS

degree: The maximum degree of interaction. Default is 1, 
use 2 for first-order interactions of the hinge functions.

First-order interaction mean products of hinge function without squaring
any thing, e.g. $f(x) = ax_1 + bx_2 + cx_1x_2 + d$.

From “Notes on the earth package”:

"For interaction terms, each variable gets credit for the entire term — thus
interaction terms are counted more than once and get a total higher weighting 
than additive terms (questionably). Each variable gets equal credit in interaction
terms even though one variable in that term may be far more important than the
other."

```{r}
degrees <- seq.int(1, 4)
deg.test <- map(degrees, function(n) {
  misty.views %>%
    run_misty(model.function = bagged_earth_model, degree = n) %>%
    collect_results()
})
deg.test.performance <- map2_dfr(deg.test, degrees, 
                            ~ .x$improvements %>% mutate(degree = .y))
```

Adding interactions terms improves the performance of MARS a lot! But does
this go at the cost of reducing the interpretability (see the above note).

However, I think it may be smart to change the default degree to 2.

```{r}
deg.test.performance %>%
  filter(measure == "multi.R2") %>%
  mutate(degree = factor(degree, levels = degrees), 
         sample = NULL, measure = NULL) %>%
  ggplot() +
  geom_point(aes(x = target, y = value, col = degree))
```


## Broken NN Function

This neural net implementation used to work, when all of a sudded I got
this error: "Error in neurons[[i]] %*% weights[[i]] : requires numeric/complex matrix/vector arguments" in the prediction part although all the input
was the right size and the predictor columns were present.

```{r}
#' Neural Network Implementation
#' 
#' @export
old_nn_model = function(view_data, target, seed, approx = TRUE, 
                         approx.frac = 0.2, k = 10, ...) {
  
  print(paste0("---nn_model called for: ", target, " ---"))
  
  ellipsis.args <- list(...)
  
  if ("k" %in% ellipsis.args) k <- ellipsis.args$k
  if ("approx.frac" %in% ellipsis.args) approx.frac <- ellipsis.args$approx.frac
  if ("frac" %in% ellipsis.args) frac <- ellipsis.args$frac
  
  folds <- withr::with_seed(
    seed,
    caret::createFolds(seq.int(1, nrow(view_data)), k = k)
  )
  
  predictors <- colnames(view_data)[colnames(view_data) != target]
  predictor.string <- glue::glue_collapse(predictors, sep = " + ")
  c.formula <- as.formula(paste0(target, " ~ ", predictor.string))
  
  # made this an imap to track the folds!
  holdout.predictions <- purrr::imap_dfr(folds, function(holdout, i) {
    
    print(paste0("Fold: ", i))
    
    in.fold <- seq.int(1, nrow(view_data))[!(seq.int(1, nrow(view_data)) %in% holdout)]
    
    # subsampling to reduce the computational cost
    if (approx) in.fold <- in.fold[sample(1:length(in.fold), 
                                          length(in.fold)*approx.frac)]
    
    train <- view_data[in.fold, ]
    
    test <- view_data[holdout, ]
    
    
    
    # Check the the variance is all zero
    check = all(all(matrixStats::colVars(as.matrix(train)) != 0),
                all(matrixStats::colVars(as.matrix(test)) != 0),
                all(colSums(is.na(train)) == 0),
                all(colSums(is.na(test)) == 0),
                all(apply(train, 2, is.numeric)),
                all(apply(test, 2, is.numeric)))
    print(paste0("No 0 var, no NAs, no non-numeric: ", check, "; and nrows in train: ", nrow(train)))
    
    algo.arguments <- list(
      formula = c.formula,
      #as.formula(paste0(target, " ~ .")),
      data = train,
      hidden = c(10),
      linear.output = FALSE,
      lifesign = "none",
      rep = 1,
      stepmax = 1e5
    )
    
    if (!(length(ellipsis.args) == 0)) {
      algo.arguments <- merge_2(algo.arguments, ellipsis.args)
    }
    
    #str(algo.arguments)
    
    model <- do.call(neuralnet::neuralnet, algo.arguments)
    
    # create data to predict
    newdata <- test %>% dplyr::select(tidyselect::all_of(model$model.list$variables))
    
    # check test data
    check = all(all(matrixStats::colVars(as.matrix(newdata)) != 0),
                all(colSums(is.na(newdata)) == 0),
                all(apply(newdata, 2, is.numeric)))
    print(paste0("Second Check: ", check))
    
    #prediction <- compute(model, newdata)
    #prediction <- predict(model, newdata)

    # label.hat <- neuralnet:::predict.nn(
    #   object = model,
    #   newdata = newdata
    #   )
    
    label.hat <- predict(model, newdata)
    
    print(head(label.hat))
    
    #label.hat <- predict(model, test)
    
    tibble::tibble(index = holdout, prediction = label.hat)
  }) %>% dplyr::arrange(index)
  
  print("succesfully computed unbiased predictions")
  
  algo.arguments.wm <- list(
    formula = c.formula,
    #as.formula(paste0(target, "~ .")),
    data = view_data,
    hidden = c(10),
    linear.output = FALSE,
    lifesign = "none",
    rep = 1,
    stepmax = 1e5
  )
  
  if (!(length(ellipsis.args) == 0)) {
    algo.arguments.wm <- merge_2(algo.arguments.wm, ellipsis.args)
  }
  
  print("---whole model---")
  
  whole.model <- do.call(neuralnet::neuralnet, algo.arguments.wm)
  
  #print(whole.model$model.list$variables)
  
  #print(summary(whole.model))
  
  predictor <- iml::Predictor$new(
    model = whole.model, 
    data = (view_data %>% dplyr::select(tidyselect::all_of(whole.model$model.list$variables))), 
    y = (view_data %>% dplyr::pull(tidyselect::all_of(target))))
  
  imp <- iml::FeatureImp$new(predictor, loss = "mse")$results
  importances <- imp$importance
  names(importances) <- imp$feature
  
  #print(importances)
  
  print("---nn_model done---")
  
  list(unbiased.predictions = holdout.predictions, 
       importances = importances)
}
```

```{r include=FALSE, eval=FALSE}
# TRASH BELOW:
# check that neural net is working.
library(neuralnet)
library(iml)
nn <- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)
predict(nn, iris)
```

```{r include=FALSE, eval=FALSE}
# test that the function is working
test1 <- test_nn_model(view_data = misty.views$intraview$data, 
                 target = "ECM", seed = 42, k = 2)
test1$importances

target.vector <- misty.views$intraview$data %>% pull(ECM)
paraview <- misty.views$paraview.10$data %>% mutate(ECM = target.vector)

test2 <- test_nn_model(view_data = paraview, 
                 target = "ECM", seed = 42, k = 2)
test2$importances
```
